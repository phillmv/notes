@jan 12, 2008

Data Mining
  data - facts
    examples
    errors
    missing parts
  information
    patterns
    expectations

process of abstraction (making a model, hiding details) that looks for patterns in data, patterns that are useful
  - these algorithms must be robust
  - must be automatic

patterns
  - allows prediction for future situations
  - allows understanding & explanation of the situation
    - structural description of the data
      - rules
      - decision tree

DM book
  - ch1. intro
  - ch2. I/O
  - ch3. knowledge representation
  - ch4. DM algorithms
  - ch5. evaluations of the results
  - ch6. DM algorithms in details
  - ch7. data preprocessing
    - missing data
    - discretization
    - attributes (features of the data) selection
  - ch8. future applications

part 2 - Pracical running - Waikato Environment for knowledge analysis

Examples of current research applications of DM
  - in vitro fertilization (London)
    after the eggs are taken and fertilized, you need to process which 60 features are desireable
  - dairy farming in NZ
    millions of cows which have to be selected each year and 1/5 go to the butcher, 4/5s keep and there are 700 features which are used to make that decision.
    DM captures the knowledge of succesful farmers and transfer to others
----

We know possess terabytes of data, which is easy to store, but there is a gap between understanding.

Contact lens data:
  you get some features, called attributes, like
    age, spectacle prescription, astigmatism, tear production rate, recommended lenses
      young, myope, no, reduced, none
      young, ", no, normal, soft
      ", ", yes, reduced, none
      ", ", ", normal, hard
      ", hypermyope, no, normal, hard

    simplistic, since *all* possible rows are present, and there are no errors and no missing values
    3 ages, 2 spectacles, two options for astimatism, two for tear production, and two for lenses
    3 * 2 * 2 * 2 = 24 rows

    applying DM -> summarization of the table
    Example rules generated
      - if (tear prod rate = reduced) then rec = none
	else if (age = young && astigmatism = no) then rec = soft


Weather problem:
  First four are independent variables, attributes.
  Outlook, Temperature, Humidity, Windy, Play
    sunny, hot, high, false, no
    ", ", ", true, no
    overcast, ", ", false, yes
    raining, mild, ", false, yes
    ", cool, normal, false, yes

    3 * 3 * 2 * 2 = 36 possible combinations
  
  Set of rules learned
    if (outlook = sunny and humidity = high) then play = no
    if (outlook = overcast) then play = yes
    if (humidity = normal) then play = yes
    if (none of the above) then play = yes
    
    if we change temperature + humidity to numeric values
      -> numeric-attribute problem
	mixed attribute problem

      One rule might be like
      if (outlook = sunny && humidity > 83) then play = no

Classification rules - predict the target attribute's class
Association rules - rules that associate any attributes
  eg. if (temp = cool) then humidity = normal
      if (humidity = normal and windy = false) then play = yes
      if (outlook = sunny and play = no) then humidity = high
	these are 100% correct + apply to 2 or more cases
	there are ~60 association rules 100% right applying to >= 2 cases

  The contact lens problem -> 9 classification rules that summarize the table 
  another structural description is a decision tree:
    tear prod rate (reduced, normal)
      reduced - none
      normal - astimagtism(no, yes)
	no - soft
	yes - spectacle prescription(myope, hypermyope)
	  myope - hard
	  hypermyope - none
  
A numeric data set - Iris classification in 1930's by Fisher
row	sepal length	sepal width	petal length	petal wdith	type
1	5.1		3.5		1.4		0.2		Iris setuse
...
51	7.0		3.2		4.7		1.4		Iris versicolor
...
150

  Rules:
    if petal lenght < 2.4 then Iris Setuse
    if sepal width < 2.10 then Iris versicolor
    if sepal width < 2.45 and petal length < 4.55 then I. versicolor

@feb 1

Oh god it's like impossible to reach this class on time.

PRISM - a simple covering algorithm for classification rules
man he's still taking about it? or something?

---
@feb21
midterm
chapters 1,2,3,4,5(part) short answer plus questions like the quiz.

chapter 5 eval

for comparison of machine learning algorithms
  for assessment of one algorithm on a dataset

    error = wrong classification for a particular instance
    error rate = #errors / # instances

    measure error rate - training set -> "substitution" error rate
				      - not a good predictor of how
				      the model will do on new data
    
    better to measure error rate on new test data
      not involved at all in the training
    if lots of data, no problem, just split it into
	- training set -> initial model
	- [validation set] -> adjust parameters
	- test set -> measure error rate

    if not so much data so then we use cross validation

	  A B C
data set
      3 way partition
      train with | test with
      A, B		C
      B, C		B
      B, C		A

      thus we tested every instance exactly once
      count total errors/# instances

      empirically & theoretically

I need glasses I can't actually see the right side of the board.

@mar 

quadratic loss function

information loss function
  can use laplace estimators

which to use?
  both work in that the loss is minimal when the predicted prob equal the true prob
    eg. consider a test instance of class 2 and different predictions of four classes
  partly a mtter of taste

Chapter 5.7
  Counting the cost

  Sometimes the cost of error is not uniform over the classes, as we've been asuming
    predicting when a cow is in heat, predicting never is right 97% of the time, but is not useful

    similarly in loan decisions, the cost of lending to a defauler is greater than the cost of refusing a loan to a nondefaulter

    in oil slick detection problem, the cost of missing a real slick is greater than the cost of a false alarm

  Confusion matrix for three classes

  Overall success rate = diagonal sum / no. test instances

  but consider a random learning method that predicts the same number of a's, b's, c's and just in proportion to the actual number
    -> Kappa statistic
